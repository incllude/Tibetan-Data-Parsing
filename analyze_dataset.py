#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–±—Ä–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ç–∏–±–µ—Ç—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
"""

import json
import sys
from pathlib import Path
from collections import defaultdict


def analyze_dataset(data_dir: str = "tibetan_data"):
    """–ê–Ω–∞–ª–∏–∑ —Å–æ–±—Ä–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return
    
    # –ß–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata_file = data_path / "metadata.json"
    if not metadata_file.exists():
        print(f"‚ùå –§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {metadata_file}")
        return
    
    with open(metadata_file, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    print(f"\n{'='*70}")
    print(f"–ê–ù–ê–õ–ò–ó –î–ê–¢–ê–°–ï–¢–ê: {data_dir}")
    print(f"{'='*70}\n")
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_pages = len(metadata)
    pages_with_images = sum(1 for m in metadata if m.get('image_file'))
    pages_with_text = sum(1 for m in metadata if m.get('text_file'))
    fully_complete = sum(1 for m in metadata if m.get('success'))
    
    print(f"üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"  –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
    print(f"  –°—Ç—Ä–∞–Ω–∏—Ü —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {pages_with_images} ({pages_with_images/total_pages*100:.1f}%)")
    print(f"  –°—Ç—Ä–∞–Ω–∏—Ü —Å —Ç–µ–∫—Å—Ç–æ–º: {pages_with_text} ({pages_with_text/total_pages*100:.1f}%)")
    print(f"  –ü–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö: {fully_complete} ({fully_complete/total_pages*100:.1f}%)")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ–∫—Å—Ç–∞–º
    text_lengths = [m['text_length'] for m in metadata if m.get('text_length', 0) > 0]
    if text_lengths:
        avg_length = sum(text_lengths) / len(text_lengths)
        min_length = min(text_lengths)
        max_length = max(text_lengths)
        
        print(f"\nüìù –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ï–ö–°–¢–ê–ú")
        print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {min_length} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {max_length} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞: {sum(text_lengths):,} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_sources = defaultdict(int)
    for m in metadata:
        if m.get('image_source'):
            image_sources[m['image_source']] += 1
    
    if image_sources:
        print(f"\nüñºÔ∏è  –ò–°–¢–û–ß–ù–ò–ö–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô")
        for source, count in image_sources.items():
            print(f"  {source}: {count} ({count/total_pages*100:.1f}%)")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ–º–∞–º
    volumes = defaultdict(int)
    for m in metadata:
        page_id = m['page_id']
        vol = page_id.split('-')[0]
        volumes[vol] += 1
    
    if volumes:
        print(f"\nüìö –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–û–ú–ê–ú")
        for vol in sorted(volumes.keys(), key=int):
            print(f"  –¢–æ–º {vol}: {volumes[vol]} —Å—Ç—Ä–∞–Ω–∏—Ü")
    
    # –ü—Ä–∏–º–µ—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü
    print(f"\nüìÑ –ü–†–ò–ú–ï–†–´ –°–¢–†–ê–ù–ò–¶")
    for i, m in enumerate(metadata[:5], 1):
        print(f"\n  {i}. {m['page_id']}")
        print(f"     –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {'‚úì' if m.get('image_file') else '‚úó'}")
        print(f"     –¢–µ–∫—Å—Ç: {'‚úì' if m.get('text_file') else '‚úó'} ({m.get('text_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤)")
        if m.get('text_preview'):
            preview = m['text_preview'][:80] + "..." if len(m['text_preview']) > 80 else m['text_preview']
            print(f"     –ü—Ä–µ–≤—å—é: {preview}")
    
    # –§–∞–π–ª–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
    images_dir = data_path / "images"
    texts_dir = data_path / "texts"
    
    image_files = list(images_dir.glob("*.png")) if images_dir.exists() else []
    text_files = list(texts_dir.glob("*.txt")) if texts_dir.exists() else []
    
    print(f"\nüíæ –§–ê–ô–õ–û–í–ê–Ø –°–ò–°–¢–ï–ú–ê")
    print(f"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –¥–∏—Å–∫–µ: {len(image_files)}")
    print(f"  –¢–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(text_files)}")
    
    # –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
    total_size = 0
    for f in data_path.rglob("*"):
        if f.is_file():
            total_size += f.stat().st_size
    
    size_mb = total_size / (1024 * 1024)
    print(f"  –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {size_mb:.2f} –ú–ë")
    
    # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    problematic = [m for m in metadata if not m.get('success')]
    if problematic:
        print(f"\n‚ö†Ô∏è  –ü–†–û–ë–õ–ï–ú–ù–´–ï –°–¢–†–ê–ù–ò–¶–´ ({len(problematic)})")
        for m in problematic[:10]:
            print(f"  - {m['page_id']}: ", end="")
            issues = []
            if not m.get('image_file'):
                issues.append("–Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            if not m.get('text_file'):
                issues.append("–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞")
            print(", ".join(issues))
        
        if len(problematic) > 10:
            print(f"  ... –∏ –µ—â–µ {len(problematic) - 10}")
    
    print(f"\n{'='*70}\n")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if fully_complete == total_pages:
        print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–±—Ä–∞–Ω! –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ–¥–µ—Ä–∂–∞—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç—ã.")
    elif fully_complete / total_pages > 0.9:
        print("‚úì –î–∞—Ç–∞—Å–µ—Ç –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–æ–±—Ä–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ.")
    else:
        print("‚ö† –í –¥–∞—Ç–∞—Å–µ—Ç–µ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞.")
    
    print(f"\n–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {data_path.absolute()}")
    print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ —Å–æ–±—Ä–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞')
    parser.add_argument('--dir', '-d', default='tibetan_data', 
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: tibetan_data)')
    
    args = parser.parse_args()
    
    analyze_dataset(args.dir)



