#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä —Ç–∏–±–µ—Ç—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å adarshah.org
–¢–µ–∫—Å—Ç –ø–∞—Ä—Å–∏—Ç—Å—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–∫–∞—á–∏–≤–∞—é—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –ø–æ URL
"""

import asyncio
import json
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import aiohttp
from playwright.async_api import async_playwright, Page


class ImprovedTibetanScraper:
    """–ü–∞—Ä—Å–µ—Ä —Ç–∏–±–µ—Ç—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self, output_dir: str = "tibetan_data", kdb: str = "degekangyur", sutra: str = "d1",
                 delay_between_pages: float = 2.0, volume_sutras: Optional[Dict[int, str]] = None, 
                 auto_sutra: bool = False, max_sutra_attempts: int = 10, max_failed_pages: int = 5, 
                 quiet_mode: bool = False):
        self.output_dir = Path(output_dir)
        self.images_dir = self.output_dir / "images"
        self.texts_dir = self.output_dir / "texts"
        self.metadata_file = self.output_dir / "metadata.json"
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.images_dir.mkdir(parents=True, exist_ok=True)
        self.texts_dir.mkdir(parents=True, exist_ok=True)
        
        self.base_url = "https://online.adarshah.org/"
        self.images_base_url = "https://files.dharma-treasure.org/"
        self.kdb = kdb  # –ö–∞—Ç–∞–ª–æ–≥ (degekangyur, degetengyur –∏ —Ç.–¥.)
        self.sutra = sutra  # –°—É—Ç—Ä–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (d1, D1109 –∏ —Ç.–¥.)
        self.volume_sutras = volume_sutras or {}  # –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ volume -> sutra
        self.auto_sutra = auto_sutra  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä sutra
        self.max_sutra_attempts = max_sutra_attempts  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞ sutra
        self.delay_between_pages = delay_between_pages  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
        self.max_failed_pages = max_failed_pages  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ—É–¥–∞—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥—Ä—è–¥
        self.quiet_mode = quiet_mode  # –¢–∏—Ö–∏–π —Ä–µ–∂–∏–º
        self.metadata = []
        self.last_successful_sutra = sutra  # –ü–æ—Å–ª–µ–¥–Ω—è—è —É—Å–ø–µ—à–Ω–∞—è sutra
        
        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        self.current_volume = None
        self.failed_pages_in_volume = 0
        
        # –ö—ç—à –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.cached_html = None
        self.cached_page_id = None
        self.cached_available_pages = set()
        self.http_requests_saved = 0
    
    def get_sutra_for_volume(self, volume: int) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å sutra –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ volume"""
        if self.auto_sutra:
            return self.volume_sutras.get(volume, self.last_successful_sutra)
        else:
            return self.volume_sutras.get(volume, self.sutra)
    
    def increment_sutra(self, sutra: str) -> str:
        """
        –£–≤–µ–ª–∏—á–∏—Ç—å —á–∏—Å–ª–æ–≤—É—é —á–∞—Å—Ç—å sutra –Ω–∞ 1
        –ü—Ä–∏–º–µ—Ä—ã: d1 -> d2, D1109 -> D1110
        """
        import re
        match = re.match(r'^([^\d]*)(\d+)$', sutra)
        if match:
            prefix = match.group(1)
            number = int(match.group(2))
            return f"{prefix}{number + 1}"
        else:
            print(f"  ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ –∏–∑ sutra: {sutra}")
            return sutra
    
    def extract_available_pages_from_html(self, html_content: str) -> set:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ HTML"""
        import re
        available_pages = set()
        pattern = r'data-pbname="(\d+-\d+[ab])"'
        matches = re.findall(pattern, html_content)
        available_pages.update(matches)
        
        if not self.quiet_mode and available_pages:
            print(f"  üì¶ –í HTML –Ω–∞–π–¥–µ–Ω–æ {len(available_pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        return available_pages
    
    async def cache_current_page(self, page: Page, page_id: str):
        """–ö—ç—à–∏—Ä—É–µ—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            html_content = await page.content()
            self.cached_html = html_content
            self.cached_page_id = page_id
            self.cached_available_pages = self.extract_available_pages_from_html(html_content)
            
            if not self.quiet_mode:
                print(f"  üíæ HTML –∫—ç—à–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_id}")
        except Exception as e:
            print(f"  ‚ö† –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è HTML: {str(e)}")
    
    def is_page_in_cache(self, page_id: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –∫—ç—à–µ"""
        return page_id in self.cached_available_pages
    
    async def load_cached_html_to_page(self, page: Page, page_id: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π HTML –≤ Playwright —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            if self.cached_html:
                await page.set_content(self.cached_html, wait_until='domcontentloaded')
                await page.wait_for_timeout(1000)
                
                if not self.quiet_mode:
                    print(f"  ‚ôªÔ∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π HTML")
                
                self.http_requests_saved += 1
                return True
        except Exception as e:
            print(f"  ‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {str(e)}")
        
        return False
        
    async def wait_for_page_load(self, page: Page, timeout: int = 30000):
        """–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            await page.wait_for_load_state('networkidle', timeout=timeout)
            await page.wait_for_timeout(2000)
        except Exception as e:
            print(f"  ‚ö† –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
    
    async def extract_tibetan_text(self, page: Page, page_id: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–±–µ—Ç—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            text_data = await page.evaluate("""
                (pageId) => {
                    // –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º page_id –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è jp –º–∞—Ä–∫–µ—Ä–∞
                    const parts = pageId.split('-');
                    let jpId;
                    if (parts.length === 2) {
                        jpId = parts[0] + '-' + parts[1].slice(0, -1) + '-' + parts[1];
                    } else {
                        jpId = pageId;
                    }
                    
                    // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–∞–∑—Ä—ã–≤–æ–≤ —Å—Ç—Ä–æ–∫
                    function extractTextWithLineBreaks(element) {
                        let text = '';
                        const childNodes = element.childNodes;
                        
                        for (let node of childNodes) {
                            if (node.nodeType === Node.TEXT_NODE) {
                                text += node.textContent;
                            } else if (node.nodeType === Node.ELEMENT_NODE) {
                                if (node.classList && node.classList.contains('ln') && 
                                    node.classList.contains('break')) {
                                    text += '\\n';
                                } else {
                                    text += extractTextWithLineBreaks(node);
                                }
                            }
                        }
                        return text;
                    }
                    
                    // –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º <jp>
                    const jpStart = document.querySelector(`jp[id="${jpId}"]`);
                    let textByJp = '';
                    
                    if (jpStart) {
                        let currentNode = jpStart.nextSibling;
                        while (currentNode) {
                            if (currentNode.nodeName === 'JP') {
                                break;
                            }
                            if (currentNode.nodeType === Node.TEXT_NODE) {
                                textByJp += currentNode.textContent;
                            } else if (currentNode.nodeType === Node.ELEMENT_NODE) {
                                textByJp += extractTextWithLineBreaks(currentNode);
                            }
                            currentNode = currentNode.nextSibling;
                        }
                    }
                    
                    // –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ –∞—Ç—Ä–∏–±—É—Ç—É data-pbname
                    const textElements = document.querySelectorAll(`span.text-pb[data-pbname="${pageId}"]`);
                    let textByAttr = '';
                    
                    textElements.forEach(el => {
                        textByAttr += extractTextWithLineBreaks(el);
                    });
                    
                    // –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                    function cleanText(text) {
                        text = text.replace(/\\d+-\\d+[ab]/g, '');
                        text = text.split('\\n').map(line => line.replace(/\\s+/g, ' ').trim()).join('\\n');
                        text = text.split('\\n').filter(line => line.length > 0).join('\\n');
                        return text.trim();
                    }
                    
                    let finalText = '';
                    let method = '';
                    
                    if (textByJp && /[\u0F00-\u0FFF]/.test(textByJp)) {
                        finalText = cleanText(textByJp);
                        method = 'jp-markers';
                    } else if (textByAttr && /[\u0F00-\u0FFF]/.test(textByAttr)) {
                        finalText = cleanText(textByAttr);
                        method = 'data-pbname';
                    }
                    
                    if (finalText) {
                        return {
                            text: finalText,
                            method: method,
                            jp_id: jpId,
                            elements_found: textElements.length
                        };
                    }
                    return null;
                }
            """, page_id)
            
            if text_data and text_data.get('text'):
                if not self.quiet_mode:
                    print(f"  ‚Ñπ –ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {text_data['method']}")
                return text_data['text']
            
            return None
            
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            return None
    
    def get_image_url(self, page_id: str) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º volume –∏–∑ page_id (—Ñ–æ—Ä–º–∞—Ç: "12-2b")
        parts = page_id.split('-')
        volume = parts[0]
        page = parts[1]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL: https://files.dharma-treasure.org/{kdb}/{kdb}{volume}-1/{volume}-1-{page}.jpg
        image_url = f"{self.images_base_url}{self.kdb}/{self.kdb}{volume}-1/{volume}-1-{page}.jpg"
        return image_url
    
    async def download_image(self, session: aiohttp.ClientSession, page_id: str) -> bool:
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –ø—Ä—è–º–æ–º—É URL"""
        try:
            image_url = self.get_image_url(page_id)
            filename = f"{page_id}.jpg"
            
            if not self.quiet_mode:
                print(f"  ‚Üí –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_url}")
            
            async with session.get(image_url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                if response.status == 200:
                    content = await response.read()
                    filepath = self.images_dir / filename
                    with open(filepath, 'wb') as f:
                        f.write(content)
                    if not self.quiet_mode:
                        print(f"  ‚úì –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
                    return True
                else:
                    print(f"  ‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: —Å—Ç–∞—Ç—É—Å {response.status}")
                    return False
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")
            return False
    
    def save_text(self, page_id: str, text: str) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ñ–∞–π–ª"""
        try:
            filepath = self.texts_dir / f"{page_id}.txt"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(text)
            if not self.quiet_mode:
                print(f"  ‚úì –¢–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {page_id}.txt ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return True
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            return False
    
    def save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        print(f"\n‚úì –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {len(self.metadata)} –∑–∞–ø–∏—Å–µ–π")
    
    async def auto_detect_sutra_for_volume(self, page: Page, session: aiohttp.ClientSession, 
                                           volume: int) -> Optional[str]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä sutra –¥–ª—è volume"""
        if not self.quiet_mode:
            print(f"\n  üîç –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä sutra –¥–ª—è volume {volume}...")
        
        current_sutra = self.last_successful_sutra
        if not self.quiet_mode:
            print(f"  ‚Ñπ –ù–∞—á–∏–Ω–∞–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —É—Å–ø–µ—à–Ω–æ–π sutra: {current_sutra}")
        
        page_id = f"{volume}-1b"  # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Ç–æ–º–∞
        
        for attempt in range(self.max_sutra_attempts):
            try:
                url = f"{self.base_url}index.html?kdb={self.kdb}&sutra={current_sutra}&page={page_id}"
                if not self.quiet_mode:
                    print(f"  ‚Üí –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{self.max_sutra_attempts}: sutra={current_sutra}")
                
                await page.goto(url, wait_until='domcontentloaded', timeout=30000)
                time.sleep(2)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞
                text = await self.extract_tibetan_text(page, page_id)
                
                if text and len(text) > 50:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–µ–∫—Å—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π
                    if not self.quiet_mode:
                        print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Ä–∞–±–æ—á–∞—è sutra: {current_sutra}")
                    self.volume_sutras[volume] = current_sutra
                    self.last_successful_sutra = current_sutra
                    return current_sutra
                else:
                    print(f"  ‚úó Sutra {current_sutra} –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç (—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω)")
                    
            except Exception as e:
                print(f"  ‚úó Sutra {current_sutra} –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç (–æ—à–∏–±–∫–∞: {str(e)[:50]}...)")
            
            current_sutra = self.increment_sutra(current_sutra)
            time.sleep(1)
        
        print(f"  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–∞–±–æ—á—É—é sutra –ø–æ—Å–ª–µ {self.max_sutra_attempts} –ø–æ–ø—ã—Ç–æ–∫")
        return None
    
    async def scrape_page(self, page: Page, session: aiohttp.ClientSession, page_id: str, 
                         max_retries: int = 3) -> Tuple[bool, bool]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Returns:
            Tuple[bool, bool]: (success, used_cache)
        """
        volume = int(page_id.split('-')[0])
        
        # –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä sutra –¥–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–º–∞
        if self.auto_sutra and page_id == f"{volume}-1b":
            if volume in self.volume_sutras:
                if not self.quiet_mode:
                    print(f"  ‚Ñπ –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—É—é sutra –¥–ª—è volume {volume}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä")
                del self.volume_sutras[volume]
            
            detected_sutra = await self.auto_detect_sutra_for_volume(page, session, volume)
            if detected_sutra is None:
                print(f"\n  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å sutra –¥–ª—è volume {volume}")
                if not self.quiet_mode:
                    print(f"  ‚Ñπ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —É—Å–ø–µ—à–Ω—É—é sutra ({self.last_successful_sutra})")
                self.volume_sutras[volume] = self.last_successful_sutra
        
        used_cache = False
        
        for attempt in range(1, max_retries + 1):
            try:
                if attempt > 1:
                    print(f"\n  üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}")
                    time.sleep(5)
                
                if not self.quiet_mode:
                    print(f"\n{'='*60}")
                    print(f"‚Üí –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_id}")
                    print(f"{'='*60}")
                
                page_sutra = self.get_sutra_for_volume(volume)
                url = f"{self.base_url}index.html?kdb={self.kdb}&sutra={page_sutra}&page={page_id}"
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
                page_loaded_from_cache = False
                if self.is_page_in_cache(page_id):
                    if not self.quiet_mode:
                        print(f"  üéØ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ!")
                    page_loaded_from_cache = await self.load_cached_html_to_page(page, page_id)
                    if page_loaded_from_cache:
                        used_cache = True
                
                # –ï—Å–ª–∏ –Ω–µ –∏–∑ –∫—ç—à–∞ - –∑–∞–≥—Ä—É–∂–∞–µ–º
                if not page_loaded_from_cache:
                    if not self.quiet_mode:
                        print(f"  URL: {url}")
                        print(f"  Volume: {volume}, Sutra: {page_sutra}")
                    
                    await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                    await self.wait_for_page_load(page)
                    await self.cache_current_page(page, page_id)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                if not self.quiet_mode:
                    print(f"\n  ‚Üí –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞...")
                text = await self.extract_tibetan_text(page, page_id)
                
                text_saved = False
                if text:
                    if not self.quiet_mode:
                        preview = text[:150] + "..." if len(text) > 150 else text
                        print(f"  ‚Ñπ –ü—Ä–µ–≤—å—é: {preview}")
                    text_saved = self.save_text(page_id, text)
                else:
                    print(f"  ‚úó –¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    # –ü—Ä–æ–±—É–µ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å sutra –µ—Å–ª–∏ –∞–≤—Ç–æ-—Ä–µ–∂–∏–º
                    if self.auto_sutra and attempt < max_retries:
                        print(f"  üîç –ü—Ä–æ–±—É–µ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å sutra...")
                        current_sutra = page_sutra
                        
                        for sutra_attempt in range(self.max_sutra_attempts):
                            current_sutra = self.increment_sutra(current_sutra)
                            if not self.quiet_mode:
                                print(f"  ‚Üí –ü–æ–ø—ã—Ç–∫–∞ —Å sutra: {current_sutra}")
                            
                            new_url = f"{self.base_url}index.html?kdb={self.kdb}&sutra={current_sutra}&page={page_id}"
                            await page.goto(new_url, wait_until='domcontentloaded', timeout=60000)
                            await self.wait_for_page_load(page)
                            time.sleep(1)
                            
                            new_text = await self.extract_tibetan_text(page, page_id)
                            if new_text and len(new_text) > 50:
                                if not self.quiet_mode:
                                    print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Ä–∞–±–æ—á–∞—è sutra: {current_sutra}")
                                self.volume_sutras[volume] = current_sutra
                                self.last_successful_sutra = current_sutra
                                text = new_text
                                text_saved = self.save_text(page_id, text)
                                url = new_url
                                break
                        else:
                            if attempt < max_retries:
                                continue
                    elif attempt < max_retries:
                        continue
                
                # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω
                image_saved = False
                image_filename = None
                if text_saved:
                    if not self.quiet_mode:
                        print(f"\n  ‚Üí –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
                    image_saved = await self.download_image(session, page_id)
                    if image_saved:
                        image_filename = f"{page_id}.jpg"
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata_entry = {
                    'page_id': page_id,
                    'volume': volume,
                    'sutra': self.get_sutra_for_volume(volume),
                    'image_file': image_filename if image_saved else None,
                    'text_file': f"{page_id}.txt" if text_saved else None,
                    'text_length': len(text) if text else 0,
                    'text_preview': text[:200] if text else None,
                    'url': url,
                    'scraped_at': datetime.now().isoformat(),
                    'success': image_saved and text_saved,
                    'attempts': attempt
                }
                self.metadata.append(metadata_entry)
                
                success = image_saved and text_saved
                
                if success:
                    if not self.quiet_mode:
                        print(f"\n  ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")
                    return (True, used_cache)
                elif text_saved or image_saved:
                    print(f"\n  ‚ö† –°—Ç—Ä–∞–Ω–∏—Ü–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —á–∞—Å—Ç–∏—á–Ω–æ")
                    return (False, used_cache)
                else:
                    if attempt < max_retries:
                        print(f"\n  ‚ö† –ù–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–µ–Ω–æ, –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞...")
                        continue
                    else:
                        print(f"\n  ‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                        return (False, used_cache)
                
            except Exception as e:
                print(f"\n  ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}): {str(e)}")
                if attempt < max_retries:
                    import traceback
                    traceback.print_exc()
                    continue
                else:
                    import traceback
                    traceback.print_exc()
                    return (False, used_cache)
        
        return (False, used_cache)
    
    def generate_page_ids(self, start_vol: int, end_vol: int, start_page: int, end_page: int) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ ID —Å—Ç—Ä–∞–Ω–∏—Ü (—Ñ–æ—Ä–º–∞—Ç: {vol}-{page}{a/b})"""
        page_ids = []
        for vol in range(start_vol, end_vol + 1):
            for page_num in range(start_page, end_page + 1):
                if page_num == 1:
                    page_ids.append(f"{vol}-{page_num}b")  # –¢–æ–ª—å–∫–æ 1b
                else:
                    page_ids.append(f"{vol}-{page_num}a")
                    page_ids.append(f"{vol}-{page_num}b")
        return page_ids
    
    async def run(self, page_ids: Optional[List[str]] = None, max_pages: Optional[int] = None, 
                  headless: bool = True):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
        if page_ids is None:
            page_ids = self.generate_page_ids(1, 1, 1, 5)
        
        if max_pages:
            page_ids = page_ids[:max_pages]
        
        print(f"\n{'#'*60}")
        print(f"# –ü–ê–†–°–ï–† –¢–ò–ë–ï–¢–°–ö–ò–• –¢–ï–ö–°–¢–û–í")
        print(f"{'#'*60}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(page_ids)}")
        print(f"–ö–∞—Ç–∞–ª–æ–≥: {self.kdb}")
        
        if self.auto_sutra:
            print(f"–†–µ–∂–∏–º sutra: –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô")
            print(f"  –ù–∞—á–∞–ª—å–Ω–∞—è sutra: {self.sutra}")
            print(f"  –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞: {self.max_sutra_attempts}")
        elif self.volume_sutras:
            print(f"–°—É—Ç—Ä—ã –ø–æ volume:")
            for vol, sutra in sorted(self.volume_sutras.items()):
                print(f"  Volume {vol}: {sutra}")
            print(f"–°—É—Ç—Ä–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {self.sutra}")
        else:
            print(f"–°—É—Ç—Ä–∞: {self.sutra}")
        
        print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –≤—ã–≤–æ–¥–∞: {self.output_dir.absolute()}")
        print(f"–†–µ–∂–∏–º –±—Ä–∞—É–∑–µ—Ä–∞: {'headless' if headless else 'visible'}")
        print(f"–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É HTTP –∑–∞–ø—Ä–æ—Å–∞–º–∏: {self.delay_between_pages} —Å–µ–∫")
        print(f"–õ–∏–º–∏—Ç –Ω–µ—É–¥–∞—á –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ volume: {self.max_failed_pages} —Å—Ç—Ä–∞–Ω–∏—Ü")
        if self.quiet_mode:
            print(f"–†–µ–∂–∏–º –≤—ã–≤–æ–¥–∞: –¢–ò–•–ò–ô")
        print(f"{'#'*60}\n")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=headless)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            )
            page = await context.new_page()
            
            if not self.quiet_mode:
                page.on("console", lambda msg: print(f"  [Browser] {msg.text}"))
            
            async with aiohttp.ClientSession() as session:
                success_count = 0
                partial_count = 0
                fail_count = 0
                skip_until_next_volume = False
                skipped_count = 0
                
                for i, page_id in enumerate(page_ids, 1):
                    volume = int(page_id.split('-')[0])
                    
                    if self.current_volume != volume:
                        self.current_volume = volume
                        self.failed_pages_in_volume = 0
                        skip_until_next_volume = False
                    
                    if skip_until_next_volume:
                        print(f"\n[{i}/{len(page_ids)}] ‚è≠ –ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_id}")
                        skipped_count += 1
                        continue
                    
                    print(f"\n[{i}/{len(page_ids)}]")
                    
                    try:
                        success, used_cache = await self.scrape_page(page, session, page_id)
                        
                        if success:
                            success_count += 1
                            self.failed_pages_in_volume = 0
                        else:
                            if self.metadata and self.metadata[-1].get('success'):
                                partial_count += 1
                                self.failed_pages_in_volume = 0
                            else:
                                fail_count += 1
                                self.failed_pages_in_volume += 1
                                
                                if self.failed_pages_in_volume >= self.max_failed_pages:
                                    print(f"\n  ‚ö† –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –Ω–µ—É–¥–∞—á –¥–ª—è volume {volume}")
                                    print(f"  ‚è≠ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã volume {volume}")
                                    skip_until_next_volume = True
                        
                        if not used_cache:
                            if not self.quiet_mode:
                                print(f"  ‚è± –ó–∞–¥–µ—Ä–∂–∫–∞ {self.delay_between_pages} —Å–µ–∫...")
                            time.sleep(self.delay_between_pages)
                        else:
                            if not self.quiet_mode:
                                print(f"  ‚ö° –ü—Ä–æ–ø—É—Å–∫ –∑–∞–¥–µ—Ä–∂–∫–∏ (–∫—ç—à)")
                        
                    except KeyboardInterrupt:
                        print("\n\n‚ö† –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                        break
                    except Exception as e:
                        print(f"\n  ‚úó –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
                        fail_count += 1
                        self.failed_pages_in_volume += 1
                        
                        if self.failed_pages_in_volume >= self.max_failed_pages:
                            print(f"\n  ‚ö† –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –Ω–µ—É–¥–∞—á –¥–ª—è volume {volume}")
                            print(f"  ‚è≠ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã volume {volume}")
                            skip_until_next_volume = True
                        continue
                
                self.save_metadata()
            
            await browser.close()
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'#'*60}")
        print(f"# –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–ê–†–°–ò–ù–ì–ê")
        print(f"{'#'*60}")
        print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(page_ids)}")
        print(f"‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é —É—Å–ø–µ—à–Ω–æ: {success_count}")
        print(f"‚ö† –ß–∞—Å—Ç–∏—á–Ω–æ —É—Å–ø–µ—à–Ω–æ: {partial_count}")
        print(f"‚úó –ù–µ—É–¥–∞—á–Ω–æ: {fail_count}")
        if skipped_count > 0:
            print(f"‚è≠ –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}")
        
        total_processed = success_count + partial_count + fail_count
        if self.http_requests_saved > 0:
            print(f"\n‚ö° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø:")
            print(f"  HTTP –∑–∞–ø—Ä–æ—Å–æ–≤ —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ: {self.http_requests_saved}")
            if total_processed > 0:
                efficiency = (self.http_requests_saved / total_processed) * 100
                print(f"  –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: {efficiency:.1f}%")
        
        print(f"\n–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.output_dir.absolute()}")
        print(f"  - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {self.images_dir}")
        print(f"  - –¢–µ–∫—Å—Ç—ã: {self.texts_dir}")
        print(f"  - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {self.metadata_file}")
        print(f"{'#'*60}\n")


async def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='–ü–∞—Ä—Å–µ—Ä —Ç–∏–±–µ—Ç—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å adarshah.org',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  # –¢–µ—Å—Ç –Ω–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
  python improved_parser.py --pages 1-1b
  
  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä sutra (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
  python improved_parser.py --auto-sutra --sutra d1 --start-vol 1 --end-vol 10 --start-page 1 --end-page 50
  
  # –ü–∞—Ä—Å–∏–Ω–≥ degetengyur —Å –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä–æ–º
  python improved_parser.py --kdb degetengyur --auto-sutra --sutra D1109 --start-vol 1 --end-vol 5
  
  # –†—É—á–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ sutra –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö volumes
  python improved_parser.py --volume-sutras 1:d1 2:d2 3:d3 --start-vol 1 --end-vol 3
  
  # –¢–∏—Ö–∏–π —Ä–µ–∂–∏–º
  python improved_parser.py --auto-sutra --sutra d1 --start-vol 1 --end-vol 10 --quiet
        """
    )
    
    parser.add_argument('--output', '-o', default='tibetan_data', 
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--kdb', default='degekangyur',
                       help='–ö–∞—Ç–∞–ª–æ–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä: degekangyur, degetengyur)')
    parser.add_argument('--sutra', default='d1',
                       help='–°—É—Ç—Ä–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä: d1, D1109)')
    parser.add_argument('--volume-sutras', nargs='+', metavar='VOLUME:SUTRA',
                       help='–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ volume->sutra (–Ω–∞–ø—Ä–∏–º–µ—Ä: 1:d1 2:d2)')
    parser.add_argument('--auto-sutra', action='store_true',
                       help='–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä sutra –¥–ª—è –∫–∞–∂–¥–æ–≥–æ volume')
    parser.add_argument('--max-sutra-attempts', type=int, default=10,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞ sutra (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)')
    parser.add_argument('--max-failed-pages', type=int, default=5,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ—É–¥–∞—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥—Ä—è–¥ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)')
    parser.add_argument('--delay', type=float, default=2.0,
                       help='–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É HTTP –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2.0)')
    parser.add_argument('--start-vol', type=int, default=1, 
                       help='–ù–∞—á–∞–ª—å–Ω—ã–π —Ç–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1)')
    parser.add_argument('--end-vol', type=int, default=1, 
                       help='–ö–æ–Ω–µ—á–Ω—ã–π —Ç–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1)')
    parser.add_argument('--start-page', type=int, default=1, 
                       help='–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1)')
    parser.add_argument('--end-page', type=int, default=5, 
                       help='–ö–æ–Ω–µ—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)')
    parser.add_argument('--max-pages', type=int, 
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü')
    parser.add_argument('--pages', nargs='+', 
                       help='–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: 1-1b 1-2a)')
    parser.add_argument('--no-headless', action='store_true',
                       help='–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)')
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='–¢–∏—Ö–∏–π —Ä–µ–∂–∏–º: –≤—ã–≤–æ–¥–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏ –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è')
    
    args = parser.parse_args()
    
    # –ü–∞—Ä—Å–∏–º volume-sutras
    volume_sutras = {}
    if args.volume_sutras:
        for mapping in args.volume_sutras:
            try:
                volume_str, sutra = mapping.split(':')
                volume = int(volume_str)
                volume_sutras[volume] = sutra
            except ValueError:
                print(f"‚ö† –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {mapping}. –û–∂–∏–¥–∞–µ—Ç—Å—è VOLUME:SUTRA")
                continue
    
    scraper = ImprovedTibetanScraper(
        output_dir=args.output, 
        kdb=args.kdb, 
        sutra=args.sutra,
        delay_between_pages=args.delay,
        volume_sutras=volume_sutras,
        auto_sutra=args.auto_sutra,
        max_sutra_attempts=args.max_sutra_attempts,
        max_failed_pages=args.max_failed_pages,
        quiet_mode=args.quiet
    )
    
    if args.pages:
        page_ids = args.pages
    else:
        page_ids = scraper.generate_page_ids(
            args.start_vol, args.end_vol,
            args.start_page, args.end_page
        )
    
    await scraper.run(
        page_ids=page_ids, 
        max_pages=args.max_pages,
        headless=not args.no_headless
    )


if __name__ == "__main__":
    asyncio.run(main())
