#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ adarshah.org
–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —ç—Ç–æ–≥–æ —Å–∞–π—Ç–∞
"""

import asyncio
import json
import os
import re
import base64
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import aiohttp
from playwright.async_api import async_playwright, Page, Browser, ElementHandle
from urllib.parse import urljoin, urlparse, parse_qs


class ImprovedTibetanScraper:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å —Ç–æ—á–Ω—ã–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–æ–≤"""
    
    def __init__(self, output_dir: str = "tibetan_data", kdb: str = "degekangyur", sutra: str = "d1",
                 image_format: str = "png", jpeg_quality: int = 95):
        self.output_dir = Path(output_dir)
        self.images_dir = self.output_dir / "images"
        self.texts_dir = self.output_dir / "texts"
        self.metadata_file = self.output_dir / "metadata.json"
        self.raw_dir = self.output_dir / "raw_html"  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.images_dir.mkdir(parents=True, exist_ok=True)
        self.texts_dir.mkdir(parents=True, exist_ok=True)
        self.raw_dir.mkdir(parents=True, exist_ok=True)
        
        self.base_url = "https://online.adarshah.org/"
        self.kdb = kdb  # –ö–∞—Ç–∞–ª–æ–≥ (degekangyur, degetengyur –∏ —Ç.–¥.)
        self.sutra = sutra  # –°—É—Ç—Ä–∞ (d1, D1109 –∏ —Ç.–¥.)
        self.image_format = image_format.lower()  # 'png' –∏–ª–∏ 'jpeg'
        self.jpeg_quality = jpeg_quality  # –ö–∞—á–µ—Å—Ç–≤–æ JPEG (1-100)
        self.metadata = []
        
    async def wait_for_page_load(self, page: Page, timeout: int = 30000):
        """–û–∂–∏–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            await page.wait_for_load_state('networkidle', timeout=timeout)
            
            # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è canvas –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø—Ä–∏–∑–Ω–∞–∫ —Ç–æ–≥–æ, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –∑–∞–≥—Ä—É–∑–∏–ª—Å—è)
            try:
                await page.wait_for_selector('canvas, img[src*="jpg"], img[src*="png"]', 
                                             timeout=15000, state='visible')
                print(f"  ‚úì –ö–æ–Ω—Ç–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω")
            except Exception:
                print(f"  ‚ö† Canvas/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –ø–æ—è–≤–∏–ª–∏—Å—å –≤ —Ç–µ—á–µ–Ω–∏–µ 15 —Å–µ–∫")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞
            await page.wait_for_timeout(5000)
        except Exception as e:
            print(f"  ‚ö† –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
    
    async def extract_image_from_canvas(self, page: Page) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ canvas —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è canvas –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
            if self.image_format == 'jpeg':
                mime_type = 'image/jpeg'
                quality = self.jpeg_quality / 100.0
            else:
                mime_type = 'image/png'
                quality = 1.0  # PNG –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç quality, –Ω–æ –ø–µ—Ä–µ–¥–∞–µ–º –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ canvas –∏ –ø–æ–ª—É—á–∏—Ç—å –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            canvas_data = await page.evaluate("""
                ({mimeType, quality}) => {
                    const canvas = document.querySelector('canvas');
                    if (canvas) {
                        try {
                            if (mimeType === 'image/png') {
                                return canvas.toDataURL(mimeType);
                            } else {
                                return canvas.toDataURL(mimeType, quality);
                            }
                        } catch (e) {
                            console.error('Error getting canvas data:', e);
                            return null;
                        }
                    }
                    return null;
                }
            """, {'mimeType': mime_type, 'quality': quality})
            return canvas_data
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ canvas: {str(e)}")
            return None
    
    async def find_page_image(self, page: Page, page_id: str) -> Optional[Tuple[str, str]]:
        """
        –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (image_data, source_type) –≥–¥–µ source_type = 'canvas' | 'img' | 'screenshot'
        """
        # –°–ø–æ—Å–æ–± 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ canvas
        canvas_data = await self.extract_image_from_canvas(page)
        if canvas_data:
            return (canvas_data, 'canvas')
        
        # –°–ø–æ—Å–æ–± 2: –ü–æ–∏—Å–∫ img —ç–ª–µ–º–µ–Ω—Ç–∞
        try:
            img_src = await page.evaluate("""
                (pageId) => {
                    const images = document.querySelectorAll('img');
                    for (const img of images) {
                        const src = img.src || img.dataset.src || '';
                        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                        if (src && (
                            src.includes(pageId) || 
                            src.includes(pageId.replace('-', '')) ||
                            img.alt === pageId ||
                            img.id === pageId
                        )) {
                            return img.src;
                        }
                    }
                    // –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∫—Ä—É–ø–Ω–æ–µ
                    for (const img of images) {
                        if (img.width > 300 && img.height > 300) {
                            return img.src;
                        }
                    }
                    return null;
                }
            """, page_id)
            
            if img_src:
                return (img_src, 'img')
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ img: {str(e)}")
        
        # –°–ø–æ—Å–æ–± 3: –°–∫—Ä–∏–Ω—à–æ—Ç –≤–∏–¥–∏–º–æ–π –æ–±–ª–∞—Å—Ç–∏ —Å —Ç–µ–∫—Å—Ç–æ–º
        try:
            # –ù–∞—Ö–æ–¥–∏–º –æ–±–ª–∞—Å—Ç—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞
            element = await page.query_selector('body')
            if element:
                # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                if self.image_format == 'jpeg':
                    screenshot = await element.screenshot(type='jpeg', quality=self.jpeg_quality)
                    screenshot_b64 = base64.b64encode(screenshot).decode()
                    return (f"data:image/jpeg;base64,{screenshot_b64}", 'screenshot')
                else:
                    screenshot = await element.screenshot(type='png')
                    screenshot_b64 = base64.b64encode(screenshot).decode()
                    return (f"data:image/png;base64,{screenshot_b64}", 'screenshot')
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–∞: {str(e)}")
        
        return None
    
    async def extract_tibetan_text(self, page: Page, page_id: str) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–±–µ—Ç—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞—Ä–∫–µ—Ä—ã <jp> –∏ data-pbname –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        """
        try:
            text_data = await page.evaluate("""
                (pageId) => {
                    // –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º page_id –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è jp –º–∞—Ä–∫–µ—Ä–∞
                    // –ù–∞–ø—Ä–∏–º–µ—Ä: "1-1b" -> "1-1-1b"
                    const parts = pageId.split('-');
                    let jpId;
                    if (parts.length === 2) {
                        // –§–æ—Ä–º–∞—Ç: "1-1b" -> "1-1-1b"
                        jpId = parts[0] + '-' + parts[1].slice(0, -1) + '-' + parts[1];
                    } else {
                        jpId = pageId;
                    }
                    
                    // –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º <jp>
                    const jpStart = document.querySelector(`jp[id="${jpId}"]`);
                    let textByJp = '';
                    
                    if (jpStart) {
                        // –ù–∞—Ö–æ–¥–∏–º —Å–ª–µ–¥—É—é—â–∏–π jp –º–∞—Ä–∫–µ—Ä –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü—ã
                        let currentNode = jpStart.nextSibling;
                        while (currentNode) {
                            // –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Å–ª–µ–¥—É—é—â–∏–º jp –º–∞—Ä–∫–µ—Ä–æ–º
                            if (currentNode.nodeName === 'JP') {
                                break;
                            }
                            
                            // –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                            if (currentNode.nodeType === Node.TEXT_NODE) {
                                textByJp += currentNode.textContent;
                            } else if (currentNode.nodeType === Node.ELEMENT_NODE) {
                                textByJp += currentNode.textContent;
                            }
                            
                            currentNode = currentNode.nextSibling;
                        }
                    }
                    
                    // –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ –∞—Ç—Ä–∏–±—É—Ç—É data-pbname
                    const textElements = document.querySelectorAll(`span.text-pb[data-pbname="${pageId}"]`);
                    let textByAttr = '';
                    
                    textElements.forEach(el => {
                        textByAttr += el.textContent;
                    });
                    
                    // –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–µ–∫—Å—Ç
                    let finalText = '';
                    let method = '';
                    
                    // –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                    function cleanText(text) {
                        // –£–¥–∞–ª—è–µ–º ID —Å—Ç—Ä–∞–Ω–∏—Ü –≤–∏–¥–∞ "1-2a"
                        text = text.replace(/\\d+-\\d+[ab]/g, '');
                        // –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
                        text = text.replace(/\\s+/g, ' ');
                        return text.trim();
                    }
                    
                    if (textByJp && /[\u0F00-\u0FFF]/.test(textByJp)) {
                        finalText = cleanText(textByJp);
                        method = 'jp-markers';
                    } else if (textByAttr && /[\u0F00-\u0FFF]/.test(textByAttr)) {
                        finalText = cleanText(textByAttr);
                        method = 'data-pbname';
                    }
                    
                    if (finalText) {
                        return {
                            text: finalText,
                            method: method,
                            jp_id: jpId,
                            elements_found: textElements.length
                        };
                    }
                    
                    return null;
                }
            """, page_id)
            
            if text_data and text_data.get('text'):
                print(f"  ‚Ñπ –ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {text_data['method']}")
                print(f"  ‚Ñπ JP ID: {text_data['jp_id']}")
                if text_data.get('elements_found'):
                    print(f"  ‚Ñπ –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {text_data['elements_found']}")
                return text_data['text']
            
            return None
            
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            return None
    
    def save_image(self, image_data: str, filename: str) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ data URL –∏–ª–∏ –æ–±—ã—á–Ω–æ–≥–æ URL"""
        try:
            filepath = self.images_dir / filename
            
            if image_data.startswith('data:'):
                # –≠—Ç–æ data URL, –¥–µ–∫–æ–¥–∏—Ä—É–µ–º base64
                header, encoded = image_data.split(',', 1)
                decoded = base64.b64decode(encoded)
                with open(filepath, 'wb') as f:
                    f.write(decoded)
            else:
                # –≠—Ç–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–∂–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                with open(filepath, 'wb') as f:
                    f.write(image_data)
            
            print(f"  ‚úì –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
            return True
            
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")
            return False
    
    async def download_image_url(self, session: aiohttp.ClientSession, url: str, filename: str) -> bool:
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ URL"""
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                if response.status == 200:
                    content = await response.read()
                    filepath = self.images_dir / filename
                    with open(filepath, 'wb') as f:
                        f.write(content)
                    print(f"  ‚úì –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {filename}")
                    return True
                else:
                    print(f"  ‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: —Å—Ç–∞—Ç—É—Å {response.status}")
                    return False
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}")
            return False
    
    def save_text(self, page_id: str, text: str) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ñ–∞–π–ª"""
        try:
            filepath = self.texts_dir / f"{page_id}.txt"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(text)
            print(f"  ‚úì –¢–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {page_id}.txt ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return True
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            return False
    
    async def save_page_html(self, page: Page, page_id: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        try:
            html = await page.content()
            filepath = self.raw_dir / f"{page_id}.html"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html)
        except Exception as e:
            print(f"  ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å HTML: {str(e)}")
    
    def save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        print(f"\n‚úì –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {len(self.metadata)} –∑–∞–ø–∏—Å–µ–π")
    
    async def scrape_page(self, page: Page, session: aiohttp.ClientSession, page_id: str, 
                         max_retries: int = 3) -> bool:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        """
        for attempt in range(1, max_retries + 1):
            try:
                if attempt > 1:
                    print(f"\n  üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}")
                    await asyncio.sleep(5)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                
                print(f"\n{'='*60}")
                print(f"‚Üí –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_id}")
                print(f"{'='*60}")
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–∞—Ç–∞–ª–æ–≥–∞ –∏ —Å—É—Ç—Ä—ã
                url = f"{self.base_url}index.html?kdb={self.kdb}&sutra={self.sutra}&page={page_id}"
                print(f"  URL: {url}")
                
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º timeout
                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                await self.wait_for_page_load(page)
            
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                await self.save_page_html(page, page_id)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                print(f"\n  ‚Üí –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
                image_result = await self.find_page_image(page, page_id)
                
                image_saved = False
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
                file_extension = 'jpg' if self.image_format == 'jpeg' else 'png'
                image_filename = f"{page_id}.{file_extension}"
                image_source = None
                
                if image_result:
                    image_data, source_type = image_result
                    image_source = source_type
                    print(f"  ‚Ñπ –ò—Å—Ç–æ—á–Ω–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {source_type}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø—É—Å—Ç–æ–π —Å–∫—Ä–∏–Ω—à–æ—Ç
                    if source_type == 'screenshot':
                        print(f"  ‚ö† –ü–æ–ª—É—á–µ–Ω —Å–∫—Ä–∏–Ω—à–æ—Ç –≤–º–µ—Å—Ç–æ canvas/img - –≤–æ–∑–º–æ–∂–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å")
                        # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞, –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
                        if attempt < max_retries:
                            continue
                    
                    if source_type == 'img' and not image_data.startswith('data:'):
                        # –≠—Ç–æ URL, –Ω—É–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å
                        full_url = urljoin(self.base_url, image_data)
                        image_saved = await self.download_image_url(session, full_url, image_filename)
                    else:
                        # –≠—Ç–æ data URL –∏–ª–∏ —É–∂–µ –≥–æ—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                        image_saved = self.save_image(image_data, image_filename)
                else:
                    print(f"  ‚úó –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                    if attempt < max_retries:
                        continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                print(f"\n  ‚Üí –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞...")
                text = await self.extract_tibetan_text(page, page_id)
                
                text_saved = False
                if text:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–∞
                    preview = text[:150] + "..." if len(text) > 150 else text
                    print(f"  ‚Ñπ –ü—Ä–µ–≤—å—é: {preview}")
                    text_saved = self.save_text(page_id, text)
                else:
                    print(f"  ‚úó –¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∏ —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
                    if not image_saved and attempt < max_retries:
                        continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata_entry = {
                    'page_id': page_id,
                    'image_file': image_filename if image_saved else None,
                    'image_source': image_source,
                    'text_file': f"{page_id}.txt" if text_saved else None,
                    'text_length': len(text) if text else 0,
                    'text_preview': text[:200] if text else None,
                    'url': url,
                    'scraped_at': datetime.now().isoformat(),
                    'success': image_saved or text_saved,
                    'attempts': attempt
                }
                self.metadata.append(metadata_entry)
                
                success = image_saved and text_saved
                
                if success:
                    print(f"\n  ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")
                    return True
                elif image_saved or text_saved:
                    print(f"\n  ‚ö† –°—Ç—Ä–∞–Ω–∏—Ü–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —á–∞—Å—Ç–∏—á–Ω–æ")
                    return False
                else:
                    # –ù–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∏–ª–∏, –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
                    if attempt < max_retries:
                        print(f"\n  ‚ö† –ù–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–µ–Ω–æ, –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞...")
                        continue
                    else:
                        print(f"\n  ‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                        return False
                
            except Exception as e:
                print(f"\n  ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}): {str(e)}")
                if attempt < max_retries:
                    import traceback
                    traceback.print_exc()
                    continue
                else:
                    import traceback
                    traceback.print_exc()
                    return False
        
        return False
    
    def generate_page_ids(self, start_vol: int, end_vol: int, start_page: int, end_page: int) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ ID —Å—Ç—Ä–∞–Ω–∏—Ü
        –§–æ—Ä–º–∞—Ç: {vol}-{page}{a/b}
        """
        page_ids = []
        for vol in range(start_vol, end_vol + 1):
            for page_num in range(start_page, end_page + 1):
                page_ids.append(f"{vol}-{page_num}a")
                page_ids.append(f"{vol}-{page_num}b")
        return page_ids
    
    async def run(self, page_ids: Optional[List[str]] = None, max_pages: Optional[int] = None, 
                  headless: bool = True):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞
        """
        if page_ids is None:
            page_ids = self.generate_page_ids(1, 1, 1, 5)
        
        if max_pages:
            page_ids = page_ids[:max_pages]
        
        print(f"\n{'#'*60}")
        print(f"# –ü–ê–†–°–ï–† –¢–ò–ë–ï–¢–°–ö–ò–• –¢–ï–ö–°–¢–û–í")
        print(f"{'#'*60}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(page_ids)}")
        print(f"–ö–∞—Ç–∞–ª–æ–≥: {self.kdb}, –°—É—Ç—Ä–∞: {self.sutra}")
        print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –≤—ã–≤–æ–¥–∞: {self.output_dir.absolute()}")
        print(f"–§–æ—Ä–º–∞—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {self.image_format.upper()}" + 
              (f" (–∫–∞—á–µ—Å—Ç–≤–æ: {self.jpeg_quality}%)" if self.image_format == 'jpeg' else ""))
        print(f"–†–µ–∂–∏–º –±—Ä–∞—É–∑–µ—Ä–∞: {'headless' if headless else 'visible'}")
        print(f"{'#'*60}\n")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=headless)
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            page = await context.new_page()
            
            async with aiohttp.ClientSession() as session:
                success_count = 0
                partial_count = 0
                fail_count = 0
                
                for i, page_id in enumerate(page_ids, 1):
                    print(f"\n[{i}/{len(page_ids)}]")
                    
                    try:
                        success = await self.scrape_page(page, session, page_id)
                        
                        if success:
                            success_count += 1
                        else:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±—ã–ª –ª–∏ —Ö–æ—Ç—å –∫–∞–∫–æ–π-—Ç–æ —É—Å–ø–µ—Ö
                            if self.metadata and self.metadata[-1].get('success'):
                                partial_count += 1
                            else:
                                fail_count += 1
                        
                        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                        await asyncio.sleep(2)
                        
                    except KeyboardInterrupt:
                        print("\n\n‚ö† –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                        break
                    except Exception as e:
                        print(f"\n  ‚úó –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
                        fail_count += 1
                        continue
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                self.save_metadata()
            
            await browser.close()
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'#'*60}")
        print(f"# –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–ê–†–°–ò–ù–ì–ê")
        print(f"{'#'*60}")
        print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(page_ids)}")
        print(f"‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é —É—Å–ø–µ—à–Ω–æ: {success_count}")
        print(f"‚ö† –ß–∞—Å—Ç–∏—á–Ω–æ —É—Å–ø–µ—à–Ω–æ: {partial_count}")
        print(f"‚úó –ù–µ—É–¥–∞—á–Ω–æ: {fail_count}")
        print(f"\n–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.output_dir.absolute()}")
        print(f"  - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {self.images_dir}")
        print(f"  - –¢–µ–∫—Å—Ç—ã: {self.texts_dir}")
        print(f"  - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {self.metadata_file}")
        print(f"{'#'*60}\n")


async def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Ç–∏–±–µ—Ç—Å–∫–∏—Ö –∏–µ—Ä–æ–≥–ª–∏—Ñ–æ–≤ —Å adarshah.org',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  # –¢–µ—Å—Ç –Ω–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–∫–∞—Ç–∞–ª–æ–≥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - degekangyur)
  python improved_parser.py --pages 1-1b
  
  # –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ degetengyur, —Å—É—Ç—Ä–∞ D1109
  python improved_parser.py --kdb degetengyur --sutra D1109 --pages 1-1b
  
  # –ü–∞—Ä—Å–∏–Ω–≥ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ JPEG (–º–µ–Ω—å—à–µ —Ä–∞–∑–º–µ—Ä)
  python improved_parser.py --pages 1-1b --image-format jpeg
  
  # JPEG —Å –∫–∞—á–µ—Å—Ç–≤–æ–º 85% (–µ—â–µ –º–µ–Ω—å—à–µ —Ä–∞–∑–º–µ—Ä)
  python improved_parser.py --pages 1-1b --image-format jpeg --jpeg-quality 85
  
  # –ü–∞—Ä—Å–∏–Ω–≥ –ø–µ—Ä–≤—ã—Ö 10 —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –¥—Ä—É–≥–æ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –≤ JPEG
  python improved_parser.py --kdb degetengyur --sutra D1109 --start-page 1 --end-page 5 --image-format jpeg
  
  # –ü–∞—Ä—Å–∏–Ω–≥ —Å –≤–∏–¥–∏–º—ã–º –±—Ä–∞—É–∑–µ—Ä–æ–º (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
  python improved_parser.py --kdb degetengyur --sutra D1109 --pages 1-1b --no-headless
        """
    )
    
    parser.add_argument('--output', '-o', default='tibetan_data', 
                       help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--kdb', default='degekangyur',
                       help='–ö–∞—Ç–∞–ª–æ–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä: degekangyur, degetengyur)')
    parser.add_argument('--sutra', default='d1',
                       help='–°—É—Ç—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: d1, D1109)')
    parser.add_argument('--image-format', choices=['png', 'jpeg'], default='png',
                       help='–§–æ—Ä–º–∞—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: png –∏–ª–∏ jpeg (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: png)')
    parser.add_argument('--jpeg-quality', type=int, default=95, 
                       help='–ö–∞—á–µ—Å—Ç–≤–æ JPEG –æ—Ç 1 –¥–æ 100 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 95)')
    parser.add_argument('--start-vol', type=int, default=1, 
                       help='–ù–∞—á–∞–ª—å–Ω—ã–π —Ç–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1)')
    parser.add_argument('--end-vol', type=int, default=1, 
                       help='–ö–æ–Ω–µ—á–Ω—ã–π —Ç–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1)')
    parser.add_argument('--start-page', type=int, default=1, 
                       help='–ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1)')
    parser.add_argument('--end-page', type=int, default=5, 
                       help='–ö–æ–Ω–µ—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)')
    parser.add_argument('--max-pages', type=int, 
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü')
    parser.add_argument('--pages', nargs='+', 
                       help='–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: 1-1b 1-2a)')
    parser.add_argument('--no-headless', action='store_true',
                       help='–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)')
    
    args = parser.parse_args()
    
    scraper = ImprovedTibetanScraper(
        output_dir=args.output, 
        kdb=args.kdb, 
        sutra=args.sutra,
        image_format=args.image_format,
        jpeg_quality=args.jpeg_quality
    )
    
    if args.pages:
        page_ids = args.pages
    else:
        page_ids = scraper.generate_page_ids(
            args.start_vol, args.end_vol,
            args.start_page, args.end_page
        )
    
    await scraper.run(
        page_ids=page_ids, 
        max_pages=args.max_pages,
        headless=not args.no_headless
    )


if __name__ == "__main__":
    asyncio.run(main())

